{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W3ztOiWkjdYL"
   },
   "source": [
    "This notebook serves to get additional data from the EDH XML/epidoc files. The reason is that some information is missing in the API data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mjj6VCcLkERG"
   },
   "outputs": [],
   "source": [
    "### REQUIREMENTS\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "### we do a lot of requests during the scrapping. Some of them with requests package, some of them with urllib\n",
    "import requests\n",
    "from urllib.request import urlopen \n",
    "from urllib.parse import quote  \n",
    "from bs4 import BeautifulSoup\n",
    "import xml.etree.cElementTree as ET\n",
    "import re\n",
    "\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "# to avoid errors, we sometime use time.sleep(N) before retrying a request\n",
    "import time\n",
    "# the input data have typically a json structure\n",
    "import json\n",
    "import getpass\n",
    "\n",
    "import datetime as dt\n",
    "# for simple paralel computing:\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import sddk\n",
    "\n",
    "# google sheets integration:\n",
    "import gspread\n",
    "from gspread_dataframe import get_as_dataframe, set_with_dataframe\n",
    "from google.oauth2 import service_account # based on google-auth library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sciencedata.dk username (format '123456@au.dk'): 648597@au.dk\n",
      "sciencedata.dk password: ········\n",
      "connection with shared folder established with you as its owner\n",
      "endpoint variable has been configured to: https://sciencedata.dk/files/SDAM_root/\n"
     ]
    }
   ],
   "source": [
    "conf = sddk.configure(\"SDAM_root\", \"648597@au.dk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to access gsheet, you need Google Service Account key json file\n",
    "# I have mine located in my personal space on sciencedata.dk, so I read it from there:\n",
    "\n",
    "# (1) read the file and parse its content\n",
    "try:\n",
    "    file_data = conf[0].get(\"https://sciencedata.dk/files/ServiceAccountsKey.json\").json()\n",
    "except:\n",
    "    print(\"cannot find file ServiceAccountsKey.json\")\n",
    "# (2) transform the content into crendentials object\n",
    "credentials = service_account.Credentials.from_service_account_info(file_data)\n",
    "# (3) specify your usage of the credentials\n",
    "scoped_credentials = credentials.with_scopes(['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive'])\n",
    "# (4) use the constrained credentials for authentication of gspread package\n",
    "gc = gspread.Client(auth=scoped_credentials)\n",
    "# (5) establish connection with spreadsheets specified by their url\n",
    "EDH_overview = gc.open_by_url(\"https://docs.google.com/spreadsheets/d/164MLxVcCZg95Bzf9fVyD1-iCA5V97eM3KAFllyhTvt4/edit?usp=sharing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we turn to the download section of the EDH website, where we can find zip archives containing xml files with individual inscriptions. Instead of downloading them manually, we will download them directly into our Python environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the download page\n",
    "resp = requests.get(\"https://edh-www.adw.uni-heidelberg.de/data/export\", headers={\"User-Agent\" : \"\"})\n",
    "url_text = resp.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['download/edhEpidocDump_HD000001-HD010000.zip',\n",
       " 'download/edhEpidocDump_HD010001-HD020000.zip',\n",
       " 'download/edhEpidocDump_HD020001-HD030000.zip',\n",
       " 'download/edhEpidocDump_HD030001-HD040000.zip',\n",
       " 'download/edhEpidocDump_HD040001-HD050000.zip',\n",
       " 'download/edhEpidocDump_HD050001-HD060000.zip',\n",
       " 'download/edhEpidocDump_HD060001-HD070000.zip',\n",
       " 'download/edhEpidocDump_HD070001-HD082046.zip']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract urls of individual zip archives for download\n",
    "download_urls = re.findall(\"download\\/edhEpidocDump_HD.+\", url_text)\n",
    "download_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collecting filenames for testing\n",
    "\n",
    "url_base = \"https://edh-www.adw.uni-heidelberg.de/\"\n",
    "\n",
    "url = url_base + download_urls[0]\n",
    "resp = requests.get(url, headers={\"User-Agent\" : \"\"})\n",
    "zipped = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "### names of all files within the zipped directory\n",
    "namelist = zipped.namelist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function for data parsing\n",
    "def get_data_from_filename(filename, zipped):\n",
    "    try:\n",
    "        soup = BeautifulSoup(zipped.read(filename))\n",
    "        xml_data = {} \n",
    "        idno_uri = soup.find(\"idno\", attrs={\"type\" : \"URI\"}).get_text()\n",
    "        xml_data[\"idno_uri\"] = idno_uri.rpartition(\"/\")[2]\n",
    "        xml_data[\"idno_tm\"] = soup.find(\"idno\", attrs={\"type\" : \"TM\"}).get_text()\n",
    "        placenames_refs = []\n",
    "        try: \n",
    "            placenames = soup.find_all(\"placename\")\n",
    "            for placename in placenames:\n",
    "                placenames_refs.append(placename[\"ref\"])\n",
    "        except: placenames_refs = []\n",
    "        xml_data[\"placenames_refs\"] = placenames_refs\n",
    "        text_tag = soup.find(\"div\", attrs={\"type\" : \"edition\"})\n",
    "        xml_data[\"text_edition\"] = \" \".join(text_tag.get_text().splitlines()[1:])\n",
    "        xml_data[\"origdate_text\"] = soup.find(\"origdate\").get_text().replace(\"\\n\", \"\")\n",
    "        try: \n",
    "            layout_execution = soup.layout.find(\"rs\")[\"ref\"]\n",
    "            xml_data[\"layout_execution\"] = layout_execution.rpartition(\"/\")[2]\n",
    "        except: xml_data[\"layout_execution\"] = \"\"\n",
    "        try: xml_data[\"layout_execution_text\"] = soup.layout.rs.get_text()\n",
    "        except: xml_data[\"layout_execution_text\"] = \"\"\n",
    "        try: \n",
    "            support_objecttype = soup.support.find(\"objecttype\")[\"ref\"]\n",
    "            xml_data[\"support_objecttype\"] = support_objecttype.rpartition(\"/\")[2]\n",
    "        except: xml_data[\"support_objecttype\"] = \"\"\n",
    "        try: xml_data [\"support_objecttype_text\"] = soup.support.objecttype.get_text()\n",
    "        except: xml_data [\"support_objecttype_text\"] = \"\"\n",
    "        try: \n",
    "            support_material = soup.support.find(\"material\")[\"ref\"]\n",
    "            xml_data[\"support_material\"] = support_material.rpartition(\"/\")[2]\n",
    "        except: xml_data[\"support_material\"] = \"\"    \n",
    "        try: xml_data[\"support_material_text\"] = soup.support.material.get_text()\n",
    "        except: xml_data[\"support_material_text\"] = \"\"\n",
    "        try: \n",
    "            support_decoration = soup.support.find(\"rs\")[\"ref\"]\n",
    "            xml_data[\"support_decoration\"] = support_decoration.rpartition(\"/\")[2]\n",
    "        except: xml_data[\"support_decoration\"] = \"\"\n",
    "        try: \n",
    "            keywords_term = soup.keywords.find(\"term\")[\"ref\"]\n",
    "            xml_data[\"keywords_term\"] = keywords_term.rpartition(\"/\")[2]\n",
    "        except: xml_data[\"keywords_term\"] = \"\"\n",
    "        try: xml_data[\"keywords_term_text\"] = soup.keywords.get_text().replace(\"\\n\", \"\")\n",
    "        except: xml_data[\"keywords_term_text\"] = \"\"\n",
    "        return xml_data\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test with first ten files within the namelist\n",
    "edh_xml_data = []\n",
    "\n",
    "for filename in namelist[:10]:\n",
    "    edh_xml_data.append(get_data_from_filename(filename, zipped))\n",
    "# transform it into dataframe\n",
    "pd.DataFrame(edh_xml_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDH_xml_cols = pd.DataFrame(pd.DataFrame(edh_xml_data).columns, columns=[\"columns\"])\n",
    "EDH_xml_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to export to gsheets\n",
    "# set_with_dataframe(EDH_overview.add_worksheet(\"EDH_xml_cols\", 1, 1), EDH_xml_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# main loop\n",
    "\n",
    "url_base = \"https://edh-www.adw.uni-heidelberg.de/\"\n",
    "edh_xml_data = []\n",
    "\n",
    "for d_url in download_urls:\n",
    "    url = url_base + d_url\n",
    "    print(url)\n",
    "    resp = requests.get(url, headers={'User-Agent': ''})\n",
    "    zipped = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "    ### names of all files within the zipped directory\n",
    "    namelist = zipped.namelist()[1:]\n",
    "    for filename in namelist:\n",
    "        try:\n",
    "            edh_xml_data.append(get_data_from_filename(filename, zipped))\n",
    "        except:\n",
    "            pass\n",
    "        ### index \"0\" is for main directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove empty\n",
    "#edh_xml_data = [elem for elem in edh_xml_data if elem != None]\n",
    "# how many we have\n",
    "# last time we had 81143\n",
    "len(edh_xml_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edh_xml_data_f = [el for el in edh_xml_data if el != None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(edh_xml_data_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dataframe from \n",
    "edh_xml_data_df = pd.DataFrame(edh_xml_data_f)\n",
    "edh_xml_data_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sddk.write_file(\"SDAM_data/EDH/edh_xml_data_2020-09-21.json\", edh_xml_data_df, conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the data back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "edh_xml_data_df = sddk.read_file(\"SDAM_data/EDH/edh_xml_data_2020-09-21.json\", \"df\", conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "1_2_py_EXTRACTION_edh-xml_files.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
