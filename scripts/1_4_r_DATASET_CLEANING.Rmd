---
title: "EDH dataset cleaning and streamlining"
author: "Petra Hermankova"
date: "16/09/2020"
output:
  html_document:
    theme: cerulean
    toc: yes
    toc_depth: 3
---

```{r setup, echo=TRUE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE, error=TRUE)
devtools::install_github("mplex/cedhar", subdir="pkg/sdam")
#install.packages("rjson")
#install.packages("tidyverse")
#install.packages("getPass")
#install.packages("formatR")

library(tidyverse)
library(sdam)
library(jsonlite)
library(getPass)
library(formatR)
```

### Loading data

1. Input your sciencedata.dk username - type directly into the RStudio console
```{r, echo = FALSE }
user <- readline("your sciencedata username: ")
```

2. Make the request (you will be asked for password in a new pop-up window)

```{r, echo = FALSE }
resp = request("EDH_merged_2020-09-15.json", path="/sharingin/648597@au.dk/SDAM_root/SDAM_data/EDH/", method="GET", cred=c(user, getPass("your sciencedata password: ")))
```


3. Make a list from the request and display the first six records (head)
```{r}
list_json <- jsonlite::fromJSON(resp)
EDH_tibble = as_tibble(list_json)
#View(EDH_tibble)
```

# Showing all attribute names before we start the cleaning process
```{r}
names(EDH_tibble)
```

# Upon inspection, we have found that there is no cleaning needed for:
1. responsible_individual
2. letter_size
3. not_after
4. literature
5. work_status
6. people
7. transcription
8. uri
9. last_update
10. language
11. id
12. edh_geography_uri
13. commentary
14. trismegistos_uri
15. not_before
16. year_of_find
17. external_image_uris
18. religion
19. fotos
20. geography
21. social_economic_legal_history
22. text_edition

## Clean the workspace to save RAM
```{r}
remove(list_json)
```



# Cleaning `type_of_inscription` attribute

## 1. Checking for consistency of the EAGLE LOD data (keywors_term) and the free-text typology (type_of_inscription).
```{r}
insc_typology <- unnest(as.data.frame(cbind(type_of_inscription=EDH_tibble$type_of_inscription, keywords_term=EDH_tibble$keywords_term)))

# compare how consistent are the LOD inscription types assigned to an epitaph (#LOD category 92 == Epitaph, 143 == Ignoratur/Unknown)
insc_typology %>% filter(type_of_inscription == "epitaph") %>% count(keywords_term, sort=TRUE)
```

## 2. Creating new column `type_of_inscription` stripped of all question marks

```{r}
EDH_clean <- EDH_tibble %>%
  mutate(type_of_inscription_clean = str_replace(EDH_tibble$type_of_inscription, pattern="\\?", replacement = ""))
```

## 3. Creating new column with `type_of_inscription_certainty` to record the uncertainty from `type_of_inscription` column

```{r}
EDH_clean$type_of_inscription_certainty <- ifelse(grepl("\\?", EDH_clean$type_of_inscription, ignore.case = T), "Uncertain", "Certain")
```

## 3. Checking the result
```{r}
number <- 20
EDH_clean$type_of_inscription[number]
EDH_clean$type_of_inscription_clean[number]
EDH_clean$type_of_inscription_certainty[number]
```


## 4. Checking there are no type of inscription types with questionmark
```{r}
EDH_clean$type_of_inscription_clean %>%
  unique() %>%
  sort(decreasing = FALSE)
`````

# Cleaning `height`, `width` and `depth` attributes

1. Cleaning the brackets
```{r}
EDH_clean <- EDH_clean %>%
  mutate(height_cm = str_replace_all(EDH_clean$height, pattern = "[()]", "")) %>%
  mutate(width_cm = str_replace_all(EDH_clean$width, pattern = "[()]", "")) %>%
  mutate(depth_cm = str_replace_all(EDH_clean$depth, pattern = "[()]", ""))

```
2. Cleaning the text and converting as numeric
```{r}
EDH_clean$height_cm <- as.numeric(str_replace(EDH_clean$height_cm, pattern = " cm", ""))
EDH_clean$width_cm <- as.numeric(str_replace(EDH_clean$width_cm, pattern = " cm", ""))
EDH_clean$depth_cm <- as.numeric(str_replace(EDH_clean$depth_cm, pattern = " cm", ""))
```
## Testing the cleaning and conversion
```{r}
dim <- c(EDH_clean$height, EDH_clean$width, EDH_clean$depth)
dims <- c(EDH_clean$height_cm, EDH_clean$width_cm, EDH_clean$depth_cm)
interval <- c(10000:10004)

# automated checks
dim[interval]
dims[interval]

# random testing if we can perform numeric operations
dims[256]+dims[8989]
```

# Cleaning `material` attribute

## 1. Checking for consistency of the EAGLE LOD data (support_material) and the free-text typology (material).
```{r}
material_typology <- unnest(as.data.frame(cbind(material=EDH_tibble$material, support_material=EDH_tibble$support_material)))

# compare how consistent are the LOD inscription types assigned to an inscription (#LOD 138 == Ignoratur/Unknown, 60 == Limestone, 75 == Sandstone, 48 == Marble)
material_typology %>% count(support_material, sort=TRUE)

# Explore what different free-text descriptions are available for LOD 48 == Marble
material_typology %>% filter(support_material == "http://www.eagle-network.eu/voc/material/lod/48") %>% unique()

```

## 2. Exploring the contents of the `material`column

```{r}
unique(EDH_clean$material)
```

## 3. Creating new column `material_clean` with main categories of the material based on Regular Expressions
```{r}
EDH_clean$material_clean <- ifelse(grepl("[Mm]ar[mor|ble]", EDH_clean$material, ignore.case = T), "Marble",
ifelse(grepl("[Ll]imestone|[Kk]alkstein|[Kk]alsktein|[Kk]alksstein", EDH_clean$material, ignore.case = T), "Limestone",
ifelse(grepl("[Aa]ndesit", EDH_clean$material, ignore.case = T), "Andesit",
ifelse(grepl("[Bb]asalt", EDH_clean$material, ignore.case = T), "Basalt",
ifelse(grepl("[Bb]reccia", EDH_clean$material, ignore.case = T), "Breccia", 
ifelse(grepl("[Dd]olomit", EDH_clean$material, ignore.case = T), "Dolomit", 
ifelse(grepl("[Gg]agat|[Jj]et", EDH_clean$material, ignore.case = T), "Jet",
ifelse(grepl("[Gg]neis", EDH_clean$material, ignore.case = T), "Gneiss",
ifelse(grepl("[Gg]ranit", EDH_clean$material, ignore.case = T), "Granit",
ifelse(grepl("[Hh][e|√É¬§|√§]matit", EDH_clean$material, ignore.case = T), "Hematit",   
ifelse(grepl("[Pp]orphyr", EDH_clean$material, ignore.case = T), "Porphyr",
ifelse(grepl("[Pp]eperin", EDH_clean$material, ignore.case = T), "Peperin",
ifelse(grepl("[Qq]uartz", EDH_clean$material, ignore.case = T), "Quartz",
ifelse(grepl("[Ss]andst[one|ein]", EDH_clean$material, ignore.case = T), "Sandstone",
ifelse(grepl("[Ss]chiefer|[Ss]late", EDH_clean$material, ignore.case = T), "Slate",       
ifelse(grepl("[Ss]teatit", EDH_clean$material, ignore.case = T), "Steatit",
ifelse(grepl("[Tt]rachyte", EDH_clean$material, ignore.case = T), "Trachyte",
ifelse(grepl("[Tt]ravertin", EDH_clean$material, ignore.case = T), "Travertine",
ifelse(grepl("[Tt]uff", EDH_clean$material, ignore.case = T), "Tuff",
ifelse(grepl("[Gg]old|[Bb]ronz|[Ss]il[vb]|[Bb]lei|[Ll]ead|[Ii]ron|[Ee]isen|[Bb]rass|[Mm]essing|[Zz]inn|[Tt]in|[Kk]upfer|[Mm]etal|[Bb]roze", EDH_clean$material, ignore.case = T), "Metal",
ifelse(grepl("[Kk]nochen|[Bb]one", EDH_clean$material, ignore.case = T), "Bone",
ifelse(grepl("[Hh]olz|[Ww]ood", EDH_clean$material, ignore.case = T), "Wood",
ifelse(grepl("[Gg]las", EDH_clean$material, ignore.case = T), "Glass",
ifelse(grepl("[Tt]on|[Cc]lay|[Pp]ottery|[Ll]ehm", EDH_clean$material, ignore.case = T), "Clay",
ifelse(grepl("[Rr]ock|[Gg]estein", EDH_clean$material, ignore.case = T), "Rock",
ifelse(grepl("[Aa]labaster", EDH_clean$material, ignore.case = T), "Alabaster",
ifelse(grepl("[Ee]lfenbein|[Ii]vory", EDH_clean$material, ignore.case = T), "Ivory",
ifelse(grepl("[Ll]e[ath|d]er", EDH_clean$material, ignore.case = T), "Leather", 
ifelse(grepl("[Bb]ernstein|[Aa]mber", EDH_clean$material, ignore.case = T), "Amber",     
ifelse(grepl("[Pp]utz|[Pp]laster", EDH_clean$material, ignore.case = T), "Plaster", 
ifelse(grepl("[Ll]ava", EDH_clean$material, ignore.case = T), "Lava",       
ifelse(grepl("[Kk]reide|[Cc]halk", EDH_clean$material, ignore.case = T), "Chalk", 
ifelse(grepl("NULL", EDH_clean$material, ignore.case = T), "NA",
                                          "Other")
                                   ))))))))))))))))))))))))))))))))
```

## 4. Comparing the `material` with `material_clean` column
```{r}
number <- c(120:125)
EDH_clean$material[number]
EDH_clean$material_clean[number]
```

## 5. Checking for unique values
```{r}
EDH_clean$material_clean %>%
  unique() %>%
  sort(decreasing = FALSE)
```

## 6. Checking what material are contained in the Other category 
```{r}
EDH_clean %>% 
  select(material, material_clean) %>% 
  filter(material_clean == "Other") %>% 
  unlist()
```


# Cleaning `type_of_monument` attribute

## 1. Checking for consistency of the EAGLE LOD data (objecttype) and the free-text typology (type_of_monument)
```{r}
object_typology <- unnest(as.data.frame(cbind(type_of_monument=EDH_tibble$type_of_monument, support_objecttype=EDH_tibble$support_objecttype)))

# compare how consistent are the LOD inscription types assigned to an inscription (#LOD 2 == Ignoratur/Unknown, 257 == Tafel/Slab, 29 == Small altar, 250 == Stele)
object_typology %>% count(support_objecttype, sort=TRUE)

# Explore what different free-text descriptions are available for LOD 48 == Marble
object_typology %>% filter(support_objecttype == "http://www.eagle-network.eu/voc/objtyp/lod/2") %>% unique()
```

## 2. Creating new column `type_of_monument_clean` stripped of all ?
```{r}
EDH_clean <- EDH_clean %>%
  mutate(type_of_monument_clean = str_replace(EDH_clean$type_of_monument, pattern="\\?", replacement = ""))
```

## 3. Creating new column with `province_label_certainty` to record the uncertainty from `province_label` column
```{r}
EDH_clean$type_of_monument_certainty <- ifelse(grepl("\\?", EDH_clean$type_of_monument, ignore.case = T), "Uncertain", "Certain")
```

## 4. Checking the result
```{r}
number <- 1334
EDH_clean$type_of_monument[number]
EDH_clean$type_of_monument_clean[number]
EDH_clean$type_of_monument_certainty[number]
```

```{r}
EDH_clean$type_of_monument_clean %>%
  unique() %>%
  sort(decreasing = FALSE)
```

# Cleaning `province_label` attribute

## 1. Checking the quality of data entry
```{r}
unique(unlist(EDH_clean$province_label))
```


## 2. Creating new column `province_label_clean` stripped of all questionmarks
```{r}
EDH_clean <- EDH_clean %>%
  mutate(province_label_clean = str_replace(EDH_tibble$province_label, pattern="\\?", replacement = ""))
```

## 3. Creating new column with `province_label_certainty` to record the uncertainty from `province_label` column
```{r}
EDH_clean$province_label_certainty <- ifelse(grepl("\\?", EDH_clean$province_label, ignore.case = T), "Uncertain", "Certain")
```
## 4. Checking the result
```{r}
number <- 856
EDH_clean$province_label[number]
EDH_clean$province_label_clean[number]
EDH_clean$province_label_certainty[number]
```

## 5. Checking there are no provinces with question mark
```{r}
EDH_clean$province_label_clean %>%
  unique() %>%
  sort(decreasing = FALSE)
```

# Cleaning `country` attribute

## 1. Checking the quality of data entry
```{r}
unique(unlist(EDH_clean$country))
```

## 2. Creating new column `country_clean` stripped of all questionmarks
```{r}
EDH_clean <- EDH_clean %>%
  mutate(country_clean = str_replace(EDH_tibble$country, pattern="\\?", replacement = ""))
```
## 3. Creating new column with `country_certainty` to record the uncertainty from `country` column
```{r}
EDH_clean$country_certainty <- ifelse(grepl("\\?", EDH_clean$country, ignore.case = T), "Uncertain", "Certain")
```
## 4. Checking the result
```{r}
number <- 1257
EDH_clean$country[number]
EDH_clean$country_clean[number]
EDH_clean$country_certainty[number]
```

## 5. Checking there are no countries with questionmark
```{r}
EDH_clean$country_clean %>%
  unique() %>%
  sort(decreasing = FALSE)
```

# Cleaning `findspot_ancient` attribute

## 1. Chekcing the quality of data entry
```{r}
unique(unlist(EDH_clean$findspot_ancient))
```




## 2. Cleaning `findspot_ancient` to a new column `findspot_ancient_clean`
```{r}
EDH_clean$findspot_ancient_clean <- str_replace(EDH_clean$findspot_ancient, pattern="[,. ] bei| aus", replacement = "")
EDH_clean$findspot_ancient_clean <- str_replace(EDH_clean$findspot_ancient_clean, pattern="\\?", replacement = "")
EDH_clean$findspot_ancient_clean <- str_replace(EDH_clean$findspot_ancient_clean, pattern=", inter|, zwischen", replacement = "")
EDH_clean$findspot_ancient_clean <- str_replace(EDH_clean$findspot_ancient_clean, pattern="(^[\\(])(.+)([\\)]$)", replacement = "\\2")
EDH_clean$findspot_ancient_clean <-str_replace(EDH_clean$findspot_ancient_clean, pattern = ",$", replacement = "")
```

# 3. Creating an index of localization certainty `findspot_ancient_certainty`
```{r}
EDH_clean$findspot_ancient_certainty <- ifelse(grepl(" bei| aus]", EDH_clean$findspot_ancient, ignore.case = T), "Estimated",
ifelse(grepl("inter|zwischen", EDH_clean$findspot_ancient, ignore.case = T), "In between",
ifelse(grepl("\\?", EDH_clean$findspot_ancient, ignore.case = T), "Uncertain",
ifelse(grepl("^\\(", EDH_clean$findspot_ancient, ignore.case = T), "Uncertain Name",
ifelse(grepl("NULL", EDH_clean$findspot_ancient, ignore.case = T), "NULL",
       "Certain" )))))
```

## 4. Checking the results
```{r}
interval <- c(10400:10410)
EDH_clean$findspot_ancient[interval]
EDH_clean$findspot_ancient_clean[interval]
EDH_clean$findspot_ancient_certainty[interval]
```

### 4.1. Checking the success rate of cleaning `findspot_ancient`
```{r}
cleaned_findspot <- unique(EDH_clean$findspot_ancient_clean) %>%
  length()
original_findspot <- unique(EDH_clean$findspot_ancient) %>%
  length()
cleaning_rate <- 100 - (cleaned_findspot /(original_findspot/100))
cleaning_rate
```
## 5. Listing all cleaned values
```{r}
EDH_clean$findspot_ancient_clean %>%
  unique() %>%
  sort(decreasing = TRUE)
```


# Cleaning `modern_region` attribute

## 1. Checking the quality of data entry

```{r}
unique(unlist(EDH_clean$modern_region))
```
## 2. Creating new column `modern_region_clean` stripped of all questionmarks
```{r}
EDH_clean <- EDH_clean %>%
  mutate(modern_region_clean = str_replace(EDH_tibble$modern_region, pattern="\\?", replacement = ""))
```
## 3. Creating new column with `modern_region_certainty` to record the uncertainty from `country` column
```{r}
EDH_clean$modern_region_certainty <- ifelse(grepl("\\?", EDH_clean$modern_region, ignore.case = T), "Uncertain", "Certain")
```
## 4. Checking the result
```{r}
number <- 12048
EDH_clean$modern_region[number]
EDH_clean$modern_region_clean[number]
EDH_clean$modern_region_certainty[number]
```

##NOTE: There is still a lof of encoding problems possibly coming from the original source
```{r}
sort(unique(unlist(EDH_clean$modern_region_clean)))
```

# Cleaning `findspot_modern` attribute

## 1. Checking the quality of data entry

```{r}
unique(unlist(EDH_clean$findspot_modern))
```
## 2. Creating new column `finsdpot_modern_clean` stripped of all questionmarks
```{r}
EDH_clean <- EDH_clean %>%
  mutate(findspot_modern_clean = str_replace(EDH_tibble$findspot_modern, pattern="\\?", replacement = ""))
```
## 3. Creating new column with `finsdpot_modern_certainty` to record the uncertainty from `country` column
```{r}
EDH_clean$findspot_modern_certainty <- ifelse(grepl("\\?", EDH_clean$findspot_modern, ignore.case = T), "Uncertain", "Certain")
```
## 4. Checking the result
```{r}
number <- 12048
EDH_clean$findspot_modern[number]
EDH_clean$findspot_modern_clean[number]
EDH_clean$findspot_modern_certainty[number]
```

##NOTE: There is still a lof of encoding problems possibly coming from the original source
```{r}
sort(unique(unlist(EDH_clean$findspot_modern_clean)))
```

# Cleaning `findspot` attribute

## 1. Checking the quality of data entry

```{r}
unique(unlist(EDH_clean$findspot))
```
## 2. Creating new column `finsdpot_clean` stripped of all questionmarks
```{r}
EDH_clean <- EDH_clean %>%
  mutate(findspot_clean = str_replace(EDH_tibble$findspot, pattern="\\?", replacement = ""))
```
## 3. Creating new column with `finsdpot_certainty` to record the uncertainty from `country` column
```{r}
EDH_clean$findspot_certainty <- ifelse(grepl("\\?", EDH_clean$findspot, ignore.case = T), "Uncertain", 
                                       ifelse(grepl("NULL", EDH_clean$findspot, ignore.case = T), "NA", 
                                       "Certain"))
```
## 4. Checking the result
```{r}
number <- 12048
EDH_clean$findspot[number]
EDH_clean$findspot_clean[number]
EDH_clean$findspot_certainty[number]
```

##NOTE: There is still a lof of encoding problems possibly coming from the original source
```{r}
head(sort(unique(unlist(EDH_clean$findspot_clean))))
```

## Cleaning the `origdate_text` attribute

## 1. Checking the quality of data entry

```{r}
head(EDH_clean$origdate_text)
```

## 2. Cleaning multi-white spaces at the end of the string and checking the results
```{r}
EDH_clean<- EDH_clean %>% 
  mutate(origdate_text_clean = gsub(pattern="\\s+$", replacement="", x=origdate_text))
head(EDH_clean$origdate_text_clean)
```



## 1. Checking the quality of data entry `layout_execution` attribute (LOD 21 == Ignoratur, 88 = Graffito, 77 =  Stamped/Gestempelt/Ex forma)
```{r}
layout<- as.data.frame(table(unlist(EDH_clean$layout_execution)))
layout
```


## 1. Checking the quality of data entry `support_decoration` attribute (LOD 1000 == not present, 2000 == yes)
```{r}
decoration<- as.data.frame(table(unlist(EDH_clean$support_decoration)))
decoration
```

# Cleaning epigraphic text for tidy text mining analysis (word and sentence centered)

*Aim:* The main purpose of this script is to clean large collections of epigraphic texts all at once in order to create cleaned texts ready for text mining analysis. The output clean texts can be used for a) word centered text mining, also known as the tidytext approach (https://www.tidytextmining.com/) or for b) sentence centered text mining, as part of the Natural Language Processing (https://en.wikipedia.org/wiki/Natural_language_processing).

The presented cleaning process is designed as generic, fairly modular and fully customisable. Therefore, it can be with some modification used to any epigraphic corpus. Ample examples are provided to illustrate individual parts of the process, so anyone familiar with _Regular Expressions_ and _basic understanding of R_ can build their own cleaning function or modify the existing ones.

The final output of the cleaning function depends on which of the individual cleaning blocks will be used and in what sequence they will run. Each individual cleaning block represents one pattern occurring repeatedly in the text that can be searched for and modified, depending on the intended outcome. All the cleaning steps are dependent on the characteristics of the original dataset, therefore familiarity with the original dataset prior the cleaning process is recommended. Each dataset can have a different set of symbols and characters to be cleaned, thus, the cleaning blocks should be adjusted accordingly.

I have created three categories of cleaning blocks, closely linked with the methodological approach and the purpose of the cleaning process:

1. `Conservative cleaning model` producing a text as close to the original as possible
2. `Interpretive cleaning model` producing a text enriched with interpretations of the corpus editor
3. Generic cleaning of patterns common for both previous categories

_Structure of a cleaning block:_

Each of the cleaning blocks maintains the same structure, using Regular expressions to find and replace the searched term or pattern.

```regexpatternname <- c("regexpattern", "substitutionpattern")```

## 1. Cleaning blocks for the conservative model

*The aim of this model is to produce a clean text that is as close to the original text of an inscription as possible, without any editorial input.*

The cleaned output of the conservative model will be as close to the original text of an inscription as possible. In most cases it should resemble a _diplomatic edition_ of epigraphic text with spaces between words, lowercase letters, eliminated brackets and non-utf compliant symbols. The interpretive restoration, substitutions or any changes of the text as appear in the dataset, done by the editor of the epigraphic corpus, are eliminated from the conservative model.

### 1.1. Expanded abbreviations

**Aim:** All expanded abbreaviations that are in the parenthesis () will be eliminated from the clean text (substituted with "").

* Example before cleaning: ```Œë·ΩêœÅ(·ΩµŒªŒπŒøœÇ) Œü·ΩêŒ±Œª·Ω≥œÅŒπŒøœÇ```
* Example after cleaning: ```Œë·ΩêœÅ Œü·ΩêŒ±Œª·Ω≥œÅŒπŒøœÇ```

```{r}
expanded_abbreviations_conservative <- c("\\([^(]*\\)", "")
```

### 1.2. Suppresion of a text with superscripts

**Aim:** All supressions that are in the curly braces {} followed by one or more superscript digits will be eliminated from the clean text (substituted with "").

**!!!** It is crutial that block `suppresion_conservative` does not precede block `suppresion_superscripts_conservative`, otherwise the Regex pattern would not clean the text properly. This particular pattern is common for the PHI dataset and may  or may not appear in other datasets.

* Example before cleaning: ```·º±ŒµœÅŒµ·Ω∫œÇ ŒªŒ∑œÜŒ∏·Ω∂œÇ ·ΩëœÄ·Ω∞ {¬≤‚Å∂·ΩëœÄ·Ω∏}¬≤‚Å∂ œÑ·ø∂ŒΩ Œ≤Œ±œÅŒ≤·Ω±œÅœâŒΩ ```
* Example after cleaning: ```·º±ŒµœÅŒµ·Ω∫œÇ ŒªŒ∑œÜŒ∏·Ω∂œÇ ·ΩëœÄ·Ω∞  œÑ·ø∂ŒΩ Œ≤Œ±œÅŒ≤·Ω±œÅœâŒΩ ```

```{r}
suppresion_superscripts_conservative <- c("{[^}]*}[‚Å∞¬π¬≤¬≥‚Å¥‚Åµ‚Å∂‚Å∑‚Å∏‚Åπ]+", "")
```

### 1.3. Suppresion of a text

**Aim:** All curly braces {} will be eliminated from the clean text (substituted with ""), while the contents of the braces will remain in the text.

**!!!** It is crutial that block `suppresion_conservative` does not precede block `suppresion_superscripts_conservative`, otherwise the Regex pattern would not clean the text properly.

* Example before cleaning: ```Œ£ŒµŒ≤Œ±œÉœÑŒø·ø¶ œÖ·º±Œø·ø¶ {Œ∏Ã£ŒµŒø·ø¶ Œ£ŒµŒ≤Œ±œÉœÑŒø·ø¶} œÑ·ΩªœáŒ∑œÇ  ```
* Example after cleaning: ```Œ£ŒµŒ≤Œ±œÉœÑŒø·ø¶ œÖ·º±Œø·ø¶ Œ∏Ã£ŒµŒø·ø¶ Œ£ŒµŒ≤Œ±œÉœÑŒø·ø¶ œÑ·ΩªœáŒ∑œÇ  ```

```{r}
suppresion_conservative <- c("[\\{*\\}]", "")
```

### 1.4. Restoration

**Aim:** All restoration that are in the square brackets [] will be eliminated from the clean text (substituted with "").

**!!!** Beware that by eliminating the contents of the brackets you may loose some context - use at your own discretion.

* Example before cleaning: ```[Œù]Œ±ŒΩŒ± ·ºùŒªŒªŒ∑ŒΩŒøÃ£[œÇ] Œ∏œÖŒ≥·Ω±œÑŒ∑œÅ Œ∫Œ±·Ω∂ ·º° ·ºëœÑ·Ω≥œÅŒ± [Œ≥œÖŒΩ·Ω¥]```
* Example after cleaning: ```Œ±ŒΩŒ± ·ºùŒªŒªŒ∑ŒΩŒø Œ∏œÖŒ≥·Ω±œÑŒ∑œÅ Œ∫Œ±·Ω∂ ·º° ·ºëœÑ·Ω≥œÅŒ±```

```{r}
restoration_conservative <- c("\\[[^[]*\\]", "")
```

### 1.5. Substitution

**Aim:** All substitutions that are in the angular brackets <> will be eliminated from the clean text (substituted with "").

**!!!** Beware that by eliminating the contents of the brackets you may loose some context - use at your own discretion.

* Example before cleaning: ```Œ∫œâœÅŒø<ŒΩ ·ºà>ŒΩœÑŒπ·Ωπœá<ŒøœÖ> ·º° œÄŒ±œÑœÅ·Ω∂œÇ œÑŒµŒπŒº·øÜ<œÇ>```
* Example after cleaning: ```Œ∫œâœÅŒø ŒΩœÑŒπ·Ωπœá ·º° œÄŒ±œÑœÅ·Ω∂œÇ œÑŒµŒπŒº·øÜœÇ```

```{r}
substitution_conservative <- c("\\<[^<]*\\>", "")
```

### 1.6. Substitution in EDH dataset

**Aim:** All  sustitutions following the pattern "A=B" will be cleaned thw following way: B remain in the text and the equal sign and A will be eliminated from the clean text.

**!!!** Beware that by eliminating the brackets you may loose some information about the preservation of the text - use at your own discretion. The `substitution_edh_interpretive` should be run before `substitution_interpretive` block, otherwise the Regex pattern would not clean the text properly. The `substitution_interpretive` block will clean the angular brackets in the next step.

* Example before cleaning: ```pos<u=I>erunt bene merenti```
* Example after cleaning: ```pos<I>erunt bene merenti```

```{r}
substitution_edh_conservative <- c("([Œ±-œâŒë-Œ©a-zA-Z])=([Œ±-œâŒë-Œ©a-zA-Z])", "\\2")
```


## 2. Cleaning blocks for the interpretive model

*The aim of this model is to produce a clean text that is enriched with interpretations of the original text as published by the editor of the corpus. The editorial interpretations include abbreviations, restorations, substitutions and suppresions of the text.*

The output of the interpretive model will produce an epigraphic text with as many editorial suggestions, restorations, corrections, and improvements as possible to provide as much possible contents of the inscription as possible. The brackets and non-utf compliant symbols will be eliminated from the `interpretive model`.

### 2.1. Expanded abbreviations

**Aim:** All parenthesis () will be eliminated from the clean text (substituted with ""), while the contents of the parenthesis will remain in the text.

* Example before cleaning: ```Œë·ΩêœÅ(·ΩµŒªŒπŒøœÇ) Œü·ΩêŒ±Œª·Ω≥œÅŒπŒøœÇ```
* Example after cleaning: ```Œë·ΩêœÅ·ΩµŒªŒπŒøœÇ Œü·ΩêŒ±Œª·Ω≥œÅŒπŒøœÇ```

```{r}
expanded_abbreviations_interpretive <- c("[\\(*\\)]", "")
```

### 2.2. Suppresion of a text with superscripts

**Aim:** Contents found within curly braces {} followed by one or more superscript digits will substitute the word immediately preceding the curly braces with the word contained in the curly braces and the braces will be eliminated, see example. Note: The cleaning block will not work if there is no text preceeding the curly braces (the pattern will be skipped).

**!!!** This particular pattern is common for the PHI dataset and may  or may not appear in other datasets. It is recommended to run the ```suppresion_keep_interpretive``` or ```suppresion_remove_interpretive``` block after ```suppresion_superscripts_interpretive``` block, otherwise the Regex pattern would not clean the text properly.

* Example before cleaning: ```·º±ŒµœÅŒµ·Ω∫œÇ ŒªŒ∑œÜŒ∏·Ω∂œÇ ·ΩëœÄ·Ω∞ {¬≤‚Å∂·ΩëœÄ·Ω∏}¬≤‚Å∂ œÑ·ø∂ŒΩ Œ≤Œ±œÅŒ≤·Ω±œÅœâŒΩ ```
* Example after cleaning: ```·º±ŒµœÅŒµ·Ω∫œÇ ŒªŒ∑œÜŒ∏·Ω∂œÇ ·ΩëœÄ·Ω∏ œÑ·ø∂ŒΩ Œ≤Œ±œÅŒ≤·Ω±œÅœâŒΩ ```

```{r}
suppresion_superscripts_interpretive <- c(" [^ ]+ \\{([‚Å∞¬π¬≤¬≥‚Å¥‚Åµ‚Å∂‚Å∑‚Å∏‚Åπ]+)([^}]+)\\}\\1", " \\2")
```

### 2.3. Suppresion of a text

**Aim:** All curly braces {} will be eliminated from the clean text (substituted with ""), while the contents of the braces will remain in the text.

**!!!** It is crutial that block ```suppresion_keep_interpretive``` or ```suppresion_remove_interpretive``` does not precede block `suppresion_superscripts_interpretive`, otherwise the Regex pattern would not clean the text properly. Due to ambiguous use of {} by editors of epigraphic corpora, the exact usage depends on the specific dataset and the way the curly braces were used. Therefore, two options how to handle curly braces are provided: If you wish to keep the text within the curly braces and remove the braces, use ```suppresion_keep_interpretive``` block. If you wish to remove the text in the braces and the braces, use ```suppresion_remove_interpretive``` block.

* Example before cleaning: ```Œ∏Ã£ŒµŒø·ø¶ Œ£ŒµŒ≤Œ±œÉœÑŒø·ø¶ œÖ·º±Œø·ø¶ {Œ∏Ã£ŒµŒø·ø¶ Œ£ŒµŒ≤Œ±œÉœÑŒø·ø¶} œÑ·ΩªœáŒ∑œÇ  ```
* Example after cleaning (keep text): ```Œ∏Ã£ŒµŒø·ø¶ Œ£ŒµŒ≤Œ±œÉœÑŒø·ø¶ œÖ·º±Œø·ø¶ Œ∏Ã£ŒµŒø·ø¶ Œ£ŒµŒ≤Œ±œÉœÑŒø·ø¶ œÑ·ΩªœáŒ∑œÇ```
* Example after cleaning (remove text): ```Œ∏Ã£ŒµŒø·ø¶ Œ£ŒµŒ≤Œ±œÉœÑŒø·ø¶ œÖ·º±Œø·ø¶  œÑ·ΩªœáŒ∑œÇ```

```{r}
suppresion_keep_interpretive <- c("[\\{*\\}]", "")
```

OR if you wish to remove the contents of the braces

```{r}
suppresion_remove_interpretive <- c("{[^}]*}", "")
```

### 2.4. Restoration

**Aim:** All square brackets [] will be eliminated from the clean text (substituted with ""), while the contents of the brackets will remain in the text.

**!!!** Beware that by eliminating the brackets you may loose some information about the preservation of the text - use at your own discretion.

* Example before cleaning: ```[Œù]Œ±ŒΩŒ± ·ºùŒªŒªŒ∑ŒΩŒøÃ£[œÇ] Œ∏œÖŒ≥·Ω±œÑŒ∑œÅ Œ∫Œ±·Ω∂ ·º° ·ºëœÑ·Ω≥œÅŒ± [Œ≥œÖŒΩ·Ω¥]```
* Example after cleaning: ```ŒùŒ±ŒΩŒ± ·ºùŒªŒªŒ∑ŒΩŒøÃ£œÇ Œ∏œÖŒ≥·Ω±œÑŒ∑œÅ Œ∫Œ±·Ω∂ ·º° ·ºëœÑ·Ω≥œÅŒ± Œ≥œÖŒΩ·Ω¥```

```{r}
restoration_interpretive <- c("[\\[*\\]]", "")
```

### 2.5. Substitution

**Aim:** All angular brackets <> will be eliminated from the clean text (substituted with ""), while the contents of the brackets will remain in the text.

**!!!** Beware that by eliminating the brackets you may loose some information about the preservation of the text - use at your own discretion.  

* Example before cleaning: ```Œ∫œâœÅŒø<ŒΩ ·ºà>ŒΩœÑŒπ·Ωπœá<ŒøœÖ> ·º° œÄŒ±œÑœÅ·Ω∂œÇ œÑŒµŒπŒº·øÜ<œÇ>```
* Example after cleaning: ```Œ∫œâœÅŒøŒΩ ·ºàŒΩœÑŒπ·ΩπœáŒøœÖ ·º° œÄŒ±œÑœÅ·Ω∂œÇ œÑŒµŒπŒº·øÜœÇ```

```{r}
substitution_interpretive <- c("[\\<*\\>]", "")
```

### 2.6. Substitution in the EDH dataset

**Aim:** All  sustitutions following the pattern "A=B" will be cleaned the following way: "A" will remain in the text and the equal sign and "B" will be eliminated from the clean text.

**!!!** The `substitution_edh_interpretive` should be run before `substitution_interpretive` block, otherwise the Regex pattern would not clean the text properly. The `substitution_interpretive` block will clean the angular brackets in the next step.

* Example before cleaning: ```pos<u=I>erunt bene merenti```
* Example after cleaning: ```pos<u>erunt bene merenti```

```{r}
substitution_edh_interpretive <- c("([Œ±-œâŒë-Œ©a-zA-Z])=([Œ±-œâŒë-Œ©a-zA-Z])", "\\1")
```


## 3. The generic text cleaning

*The aim of the generic cleaning is to strip the epigraphic text any non-utf compliant symbols and characters that do not adhere to the principles of a quantitat ive text mining.*

The cleaning blocks in this section represent common patterns appearing in any epigraphic text, such as interpunction, lacunas or other representations of an empty space, various editorial notes and comments in the text itself, that are not relevant to the text mining, erasures, numerals, and several specific unicode symbols appearing in the original text. Depending on the characteristics of the originahal dataset and the intended outcome, anyone can change individial cleaning blocks to better fit their needs. Through testing is, however, strongly recommended!

### 3.1. Lacuna 1

**Aim:** All square brackets [] containing one or more "‚Äî " will be eliminated from the clean text (substituted with "").

**!!!** The block ```lacuna1``` should be run before ```restoration_conservative``` and ```restoration_interpretive``` blocks, otherwise the Regex pattern would not clean the text properly. Note: If there is a text within the square bracket, e.g. ```œÄœÅŒø·ΩªœáŒøŒΩ[œÑŒøœÇ ‚Äî ‚Äî ‚Äî]``` the block ```lacuna1``` will skip the pattern. However, the block ```restoration_interpretive``` will eliminate the square brackets, the script ```interpunction_symbols``` will clean the "‚Äî" and the script ```multi_whitespace``` will eliminate the extra whitespaces. Therefore the blocks should be used in combination and in the indicated sequence: (1)```restoration_interpretive```, (2)```interpunction_symbols``` and (3)```multi_whitespace```.

* Example before cleaning: ```[‚Äî ‚Äî ‚Äî]Œ∑œÇ Œ∏Œµ·ø∑ Œ¶Œø·Ω∑Œ≤·ø≥```
* Example after cleaning: ```Œ∑œÇ Œ∏Œµ·ø∑ Œ¶Œø·Ω∑Œ≤·ø≥```

```{r}
lacuna1 <- c("\\[[‚Äî ]+\\]", "")
```

### 3.2. Lacuna 2

**Aim:** All square brackets [] containing one or more "." will be eliminated from the clean text (substituted with "").

**!!!** The block ```lacuna2``` should be run before ```restoration_conservative``` and ```restoration_interpretive``` blocks, otherwise the Regex pattern would not clean the text properly. Note: If there is a text within the square bracket, e.g. ```œÄœÅŒø·ΩªœáŒøŒΩ[œÑŒøœÇ...]``` the block ```lacuna2``` will skip the pattern. However, the block ```restoration_interpretive``` will eliminate the square brackets, the script ```interpunction_symbols``` will clean the "." and the script ```multi_whitespace``` will eliminate the extra whitespaces. Therefore the blocks should be used in combination and in the indicated sequence: (1)```restoration_interpretive```, (2)```interpunction_symbols``` and (3)```multi_whitespace```.

* Example before cleaning: ```[‚Ä§‚Ä§]œâ ŒîŒπ·Ω∂ Œ∫Œ±·Ω∂ ·º≠œÅ·æ≥```
* Example after cleaning: ```œâ ŒîŒπ·Ω∂ Œ∫Œ±·Ω∂ ·º≠œÅ·æ≥```

```{r}
lacuna2 <- c("\\[[‚Ä§]+\\]", "")
```

### 3.3. Vacat

**Aim:** All instances of the following strings "vacat, vac, vac., v." will be replaced by a space (substituted with " "). If there is any extra whitespace, it will be cleaned by ```multi_whitespace``` block in the following steps.

**!!!** If your datasets contains latin inscriptions, you may want to check whether the ```vacat``` block is not eliminitating more words than anticipated, e.g. words containing string "vacat" or "vac". If so, adjust the cleaning block accordingly, i.e. remove "vac", or don't use it.

* Example before cleaning: ```·º©œÅŒ±Œ∫ŒªŒµ·Ω∑Œ¥Œ± vacat œáŒ±·øñœÅŒµ.```
* Example after cleaning: ```·º©œÅŒ±Œ∫ŒªŒµ·Ω∑Œ¥Œ±    œáŒ±·øñœÅŒµ.```

```{r}
vacat <- c("(vacat|vac|vac\\.|v\\.)", " ")
```

### 3.4. Editorial notes

**Aim:** All instances of the editorial notes in parenthesis such as (vel sim.) will be replaced by a space (substituted with " "). If there is any extra whitespace, it will be cleaned by ```multi_whitespace``` block in the following steps.

**!!!** The ```editorial_notes``` block should run before the ```expanded_abbreviations_conservative``` and ```expanded_abbreviations_interpretive``` blocks, otherwise the Regex pattern would not clean the text properly.

* Example before cleaning: ```·º≠œÅœâŒπ (vel sim.) ŒöŒ±ŒªŒªŒπœÉŒ∏·Ω≥ŒΩŒ∑œÇ```
* Example after cleaning: ``·º≠œÅœâŒπ   ŒöŒ±ŒªŒªŒπœÉŒ∏·Ω≥ŒΩŒ∑œÇ```

```{r}
editorial_notes <-c("\\(vel sim.\\)", " ")
```

### 3.5. New line

**Aim:** All instances of in-line symbol for new line (|) will be eliminated (substituted with "").

* Example before cleaning: ```Œõ·Ω±ŒºœÄœÅŒ∑ Œ§Ã£ŒµŒªŒµœÉ·ΩµŒΩŒøœÅ|ŒøœÇ Œ≥œÖŒΩ·Ωµ.```
* Example after cleaning: ```Œõ·Ω±ŒºœÄœÅŒ∑ Œ§Ã£ŒµŒªŒµœÉ·ΩµŒΩŒøœÅŒøœÇ Œ≥œÖŒΩ·Ωµ```

```{r}
new_line <- c("[\\||\\/]", "")
```

### 3.6. Split word over two lines

**Aim:** All instances of words split between two lines with a dash (-) will be eliminated (substituted with "").

* Example before cleaning: ```·ºÄœÅœáŒπŒµœÅ·Ω≥œâœÇ Œ∫Œ±·Ω∂ Œµ·ΩêœÄŒøœÉŒπ·Ω±œÅ-\nœáŒøœÖ ŒºŒ∑ŒΩ·Ω∏œÇ```
* Example after cleaning: ```·ºÄœÅœáŒπŒµœÅ·Ω≥œâœÇ Œ∫Œ±·Ω∂ Œµ·ΩêœÄŒøœÉŒπ·Ω±œÅœáŒøœÖ ŒºŒ∑ŒΩ·Ω∏œÇ```

```{r}
split_word_multiline <- c("-\\n", "")
```

### 3.7. Erasure empty

**Aim:** All instances of erased text („Äö‚Äî„Äõ) will be replaced by a space (substituted with " "). If there is any extra whitespace, it will be cleaned by ```multi_whitespace``` block in the following steps.

* Example before cleaning: ```·ºàœÅœÑ·Ω≥ŒºŒπŒ¥Œπ „Äö‚Äî ‚Äî ‚Äî„Äõ ·ºêœÄŒ∑Œ∫·ΩπŒøŒπœÇ.```
* Example after cleaning: ```·ºàœÅœÑ·Ω≥ŒºŒπŒ¥Œπ  ·ºêœÄŒ∑Œ∫·ΩπŒøŒπœÇ.```

```{r}
erasure_empty <- c("„Äö[‚Äî ]+„Äõ", " ")
```

### 3.8. Erasure with new text

**Aim:** All instances of double brackets for erasures („Äö „Äõ) will be eliminated (substituted with "") and the contents of the double brackets will be preserved as part of the clean text.

* Example before cleaning: ```·ºàŒº·ΩªŒΩœÑœâœÅ ŒùŒøœÖŒºŒ∑ŒΩ·Ω∑ŒøœÖ „ÄöœáŒ±·øñœÅŒµ„Äõ. Œ∫Œ±·Ω∂ ·º° Œ≥œÖŒΩ·Ω¥ Œ±·ΩêœÑŒø·ø¶```
* Example after cleaning: ```·ºàŒº·ΩªŒΩœÑœâœÅ ŒùŒøœÖŒºŒ∑ŒΩ·Ω∑ŒøœÖ œáŒ±·øñœÅŒµ. Œ∫Œ±·Ω∂ ·º° Œ≥œÖŒΩ·Ω¥ Œ±·ΩêœÑŒø·ø¶```

```{r}
erasure_new_text <- c("[„Äö„Äõ]", "")
```

### 3.9. Dubious dot subscript

**Aim:** All instances of the dubious reading marked by the subscrit dot (unicode 0323) will be eliminated (substituted with "").

**!!!** The ```dubious_dot_subscript``` block should happen as first step of the cleaning, otherwise the letters might shift and the Regex pattern would not clean the text properly.

* Example before cleaning: ``` ·ºàÃ£œÄ·ΩπÃ£ŒªÃ£ŒªÃ£œâŒΩŒøœÇ```
* Example after cleaning: ``` ·ºàœÄ·ΩπŒªŒªœâŒΩŒøœÇ```

```{r}
dubious_dot_subscript <- c("\u{0323}", "")
```

### 3.10. Interpunction symbols

**Aim:** All instances of listed interpunction symbols (,.!-‚Äî#%^&\*/~:;) will be replaced by a space (substituted with " "). If there is any extra whitespace, it will be cleaned by ```multi_whitespace``` block in the following steps.

**!!!** If you wish to keep sentence separators, such as dots at the bottom of the line, use ```interpunction_keep_sentences``` or elimininate the sentence separators you want to keep in your text from the cleaning block ```interpunction_keep_sentences```.

* Example before cleaning: ```Œ¶ŒπŒª·ΩµœÑŒ∑ # Œ∏Œµ·æ∑ ŒúŒ±ŒªŒøœÜ·ΩπœÅ·ø≥``` or ```Œ∫Œµ·øñŒºŒ±Œπ œÄœÅ·ΩπŒºŒøŒπœÅŒøœÇ ·ºôœÅŒºŒøŒ≥·Ω≥ŒΩŒ∑œÇ œÑœÖŒºŒ≤ŒµœÖŒ∏Œµ·Ω∑œÇ. /·ºÄŒ≥·ΩºŒΩ```
* Example after cleaning: ```Œ¶ŒπŒª·ΩµœÑŒ∑  Œ∏Œµ·æ∑ ŒúŒ±ŒªŒøœÜ·ΩπœÅ·ø≥``` or ```Œ∫Œµ·øñŒºŒ±Œπ œÄœÅ·ΩπŒºŒøŒπœÅŒøœÇ ·ºôœÅŒºŒøŒ≥·Ω≥ŒΩŒ∑œÇ œÑœÖŒºŒ≤ŒµœÖŒ∏Œµ·Ω∑œÇ   ·ºÄŒ≥·ΩºŒΩ```
* Example after cleaning (keep sentences): ```Œ∫Œµ·øñŒºŒ±Œπ œÄœÅ·ΩπŒºŒøŒπœÅŒøœÇ ·ºôœÅŒºŒøŒ≥·Ω≥ŒΩŒ∑œÇ œÑœÖŒºŒ≤ŒµœÖŒ∏Œµ·Ω∑œÇ.  ·ºÄŒ≥·ΩºŒΩ```

```{r}
interpunction_symbols <- c("[,|\\.|‚Ä§|:|‚ãÆ|‚Åô|;|!|\\-|‚Äî|‚Äì|#|%|\\^|&|\\*|~|@]", " ")
```
OR

if you wish to preserve sentence separators, such as dots

```{r}
interpunction_keep_sentences <- c("[!|\\-|‚Äî|‚Äì|#|%|\\^|&|\\*|~|@]", " ")
```

### 3.11. Superscript numbers

**Aim:** All instances of superscripted numbers will be eliminated (substituted with "").

**!!!** The ```superscript_numbers``` should not be run before the ```suppresion_superscripts_conservative``` or ```suppresion_superscripts_interpretive``` block, otherwise the Regex pattern would not clean the text properly.

* Example before cleaning: ```Œë·ΩêœÅ(·ΩµŒªŒπŒøœÇ) ŒîŒπŒøŒΩ·ΩªœÉŒπŒøœÇ #‚Åµ‚Å∂ Œ≤Õ¥ #‚Åµ‚Å∂```
* Example after cleaning: ```Œë·ΩêœÅ(·ΩµŒªŒπŒøœÇ) ŒîŒπŒøŒΩ·ΩªœÉŒπŒøœÇ # Œ≤Õ¥ #```

```{r}
superscript_numbers <- c("[‚Å∞¬π¬≤¬≥‚Å¥‚Åµ‚Å∂‚Å∑‚Å∏‚Åπ]+", "")
```

### 3.12. Epigraphic symbols

**Aim:** All instances of the listed specialised epigraphic symbols, such as the haedera (‚ù¶), will be eliminated (substituted with "").

* Example before cleaning: ```·ºÄŒ≥Œ±Œ∏·øÜŒπ ‚ù¶ œÑ·ΩªœáŒ∑Œπ```
* Example after cleaning: ```·ºÄŒ≥Œ±Œ∏·øÜŒπ   œÑ·ΩªœáŒ∑Œπ```

```{r}
epigraphic_symbols <-c ("[‚ù¶|Œá|‚àô|êÜñ|‚èë|‚èì|‚èï]", "")
```

### 3.13. Uncertainty symbols

**Aim:** All instances of th elisted symbols marking uncertainty (?) will be eliminated (substituted with "").

* Example before cleaning: ```œáŒ±·øñœÅŒµ?```
* Example after cleaning: ```œáŒ±·øñœÅŒµ```

```{r}
uncertainty_symbols <-c ("[\\?]", "")
```

### 3.14. End of line

**Aim:** All instances of end of line symbol (\n) will be replaced by space (substituted with " ").

* Example before cleaning: ```Œ∫Œ±·Ω∂ ·ºÑœÅŒæŒ±ŒΩœÑŒ±\nœÑŒø·ø¶ Œ∫ŒøŒπŒΩŒø·ø¶```
* Example after cleaning: ```Œ∫Œ±·Ω∂ ·ºÑœÅŒæŒ±ŒΩœÑŒ± œÑŒø·ø¶ Œ∫ŒøŒπŒΩŒø·ø¶```

```{r}
end_line <- c("\\n", " ")
```

### 3.15. Extra blank space

**Aim:** All instances of extra blank space ("‚ÄÉ") will be replaced by space (substituted with " ").

* Example before cleaning: ```·ºÄŒ≥Œ±Œ∏·øÜŒπ‚ÄÉ‚ÄÉ œÑ·ΩªœáŒ∑Œπ.```
* Example after cleaning: ```·ºÄŒ≥Œ±Œ∏·øÜŒπ   œÑ·ΩªœáŒ∑Œπ.```

```{r}
extra_blank <- c("[‚ÄÉ]+", " ")
```

### 3.16. Multi-whitespace

**Aim:** All instances of more then one whitespace "  " next to each other will be eliminated (substituted with "").

**!!!** The ```multi_whitespace``` should run as the second last cleaning block to ensure all redundant white spaces are cleaned from the text.

* Example before cleaning: ```·º©œÅŒ±Œ∫ŒªŒµ·Ω∑Œ¥Œ±    œáŒ±·øñœÅŒµ.```
* Example after cleaning: ```·º©œÅŒ±Œ∫ŒªŒµ·Ω∑Œ¥Œ± œáŒ±·øñœÅŒµ.```

```{r}
multi_whitespace <- c("\\s+", " ")
```

### 3.17. Trailing and leading whitespace

**Aim:** All instances of whitespace " " at the beginning and end of the line will be eliminated (substituted with "").

**!!!** The ```whitespace_endline``` should run as the last cleaning block to ensure all redundant white spaces are cleaned from the text.

* Example before cleaning: ``` œáŒ±·øñœÅŒµ ```
* Example after cleaning: ```œáŒ±·øñœÅŒµ```

```{r}
whitespace_endline <- c("(^\\s|\\s$)", "")
```

### 3.18. Editorial comments in Latin alphabet

**Aim:** All instances of editorial comments in Latin alphabet that are enclosed in curly braces {} with superscript numbers will be eliminated (substituted with "").

**!!!** If your dataset contains Latin inscriptions, use this block with caution. Verify first, that running the block  does not eliminate any necessary information or text. This block has been specifically designed for the interpretive cleaning of the PHI Greek Inscription dataset and it should run before ```suppresion_superscripts_interpretive``` and ```suppresion_interpretive``` blocks, otherwise the Regex pattern would not clean the text properly.

* Example before cleaning: ```·ºÄŒ≥Œ±Œ∏·øÜŒπ œÑ·ΩªœáŒ∑Œπ. {¬≤in parte inferiore altera manu incisa est:}¬≤ ·ΩëœÄ·Ω≤œÅ œÑ·øÜœÇ œÑŒø·ø¶```
* Example after cleaning: ```·ºÄŒ≥Œ±Œ∏·øÜŒπ œÑ·ΩªœáŒ∑Œπ. ·ΩëœÄ·Ω≤œÅ œÑ·øÜœÇ œÑŒø·ø¶```

```{r}
editorial_comments_latin <- c("\\{([‚Å∞¬π¬≤¬≥‚Å¥‚Åµ‚Å∂‚Å∑‚Å∏‚Åπ]+)([a-zA-Z0-9][^}]+)\\}\\1", "")
```

### 3.19. Arabic numerals

**Aim:** All instances of arabic numerals (0-9) will be eliminated (substituted with "").

**!!!** If your dataset contains arabic numerals that you would like to keep, use this block with caution. Verify first, that running the block does not eliminate any necessary information or text. This block has been specifically designed for the interpretive cleaning of the PHI Greek Inscription dataset and it should run before ```multi_whitespace``` and ```whitespace_endline``` blocks, otherwise the Regex pattern would not clean the text properly.

* Example before cleaning: ```·º° Œ≥œÖŒΩ·Ω¥ Œ±·ΩêœÑŒø·ø¶ Œ¶ŒπŒªŒôÃ£ 4 5 Œ∫Œ±·Ω∂```
* Example after cleaning: ```·º° Œ≥œÖŒΩ·Ω¥ Œ±·ΩêœÑŒø·ø¶ Œ¶ŒπŒªŒô Œ∫Œ±·Ω∂```

```{r}
arabic_numerals <- c("[0-9]+", "")
```

### 3.20 Unclosed brackets

**Aim:** All instances of unclosed brackets will be eliminated (substituted with "").

**!!!** Use the `unclosed_brackets` block immediately before ```multi_whitespace``` and ```whitespace_endline``` blocks, otherwise the Regex pattern would not clean the text properly.

* Example before cleaning: ```ummio isenna Xv [```
* Example after cleaning: ```ummio isenna Xv ```

```{r}
unclosed_brackets <- c("[\\[|\\{|\\(|\\)|\\}|\\]]", "")
```

## Building cleaning functions for specific datasets

When we have established the individual buidling blocks, we can put them together in the right sequence and build a cleaning function in R for conservative and interpretive models.

### Conservative model
*Aim:* to have a clean text that is as close to the original inscription as preserved on the medium - in case of the EDH dataset column `diplomatic_text` should be similar to the output of the `conservative_cleaning` model.  

Since the dataset is mostly in Latin, I did not use the following cleaning scripts: `vacat`, `editorial_notes`, `editorial_comments_latin` since they would eliminate some parts of the text that should not be eliminated. I am not using the `suppresion_superscripts_conservative` script beacuse the structure of the EDH dataset does not contain curly braces followed by superscript numbers. The script `unclosed_brackets` has been added since EDH dataset contains a lot of unclosed brackets of all kinds. Script `substitution_edh_conservative` was added to clean additional substitution features of the EDH dataset.

```{r}
cleaning_conservative_edh <- function(epigraphic_dataset){
  clean_text <- gsub(pattern=dubious_dot_subscript[1], replacement=dubious_dot_subscript[2], x=epigraphic_dataset, perl=TRUE)
  clean_text <- gsub(pattern=lacuna1[1], replacement=lacuna1[2], x=clean_text, perl=TRUE)
  clean_text <- gsub(pattern=lacuna2[1], replacement=lacuna2[2], x=clean_text, perl=TRUE)
  clean_text <- gsub(pattern=expanded_abbreviations_conservative[1], replacement=expanded_abbreviations_conservative[2], x=clean_text, perl=TRUE)
  clean_text <- gsub(pattern=suppresion_conservative[1], replacement=suppresion_conservative[2], x=clean_text, perl=TRUE)
  clean_text <- gsub(pattern=restoration_conservative[1], replacement=restoration_conservative[2], x=clean_text, perl=TRUE)
  clean_text <- gsub(pattern=substitution_edh_conservative[1], replacement=substitution_edh_conservative[2], x=clean_text, perl=TRUE)
  clean_text <- gsub(pattern=substitution_conservative[1], replacement=substitution_conservative[2], x=clean_text, perl=TRUE)
  clean_text <- gsub(pattern=new_line[1], replacement=new_line[2], x=clean_text, perl=TRUE)
  clean_text <- gsub(pattern=split_word_multiline[1], replacement=split_word_multiline[2], x=clean_text, perl=TRUE)
  clean_text <- gsub(pattern=erasure_empty[1], replacement=erasure_empty[2], x=clean_text, perl=TRUE)
  clean_text <- gsub(pattern=erasure_new_text[1], replacement=erasure_new_text[2], x=clean_text, perl=TRUE)
  clean_text <- gsub(pattern=interpunction_symbols[1], replacement=interpunction_symbols[2], x=clean_text, perl=TRUE)
  clean_text <- gsub(pattern=superscript_numbers[1], replacement=superscript_numbers[2], x=clean_text, perl=TRUE)
  clean_text <- gsub(pattern=epigraphic_symbols[1], replacement=epigraphic_symbols[2], x=clean_text, perl=TRUE)
  clean_text <- gsub(pattern=uncertainty_symbols[1], replacement=uncertainty_symbols[2], x=clean_text, perl=TRUE)
  clean_text <- gsub(pattern=uncertainty_symbols[1], replacement=uncertainty_symbols[2], x=clean_text, perl=TRUE)
  clean_text <- gsub(pattern=end_line[1], replacement=end_line[2], x=clean_text, perl=TRUE)
  clean_text <- gsub(pattern=extra_blank[1], replacement=extra_blank[2], x=clean_text, perl=TRUE)
  clean_text <- gsub(pattern=arabic_numerals[1], replacement=arabic_numerals[2], x=clean_text, perl=TRUE)
  clean_text <- gsub(pattern=unclosed_brackets[1], replacement=unclosed_brackets[2], x=clean_text, perl=TRUE)
  clean_text <- gsub(pattern=multi_whitespace[1], replacement=multi_whitespace[2], x=clean_text, perl=TRUE)
  clean_text <- gsub(pattern=whitespace_endline[1], replacement=whitespace_endline[2], x=clean_text, perl=TRUE)
  return(clean_text)
}
```

#### Example of conservative cleaning:

`Transcription` column of the first five inscriptions before cleaning:

```{r}
print(EDH_clean$transcription[1:5])
```

`Diplomatic_text` column of the first five inscriptions (for comparison with the cleaning output):
```{r}
print(EDH_clean$diplomatic_text[1:5])
```

Output of the ```cleaning_conservative_edh``` function:

```{r}
example_edh <- as.data.frame(cleaning_conservative_edh(EDH_clean$transcription))
example_edh[1:5,]
```



### Interpretive model for 'tidytext' analysis based on the analysis of words
*Aim:* to have a clean text enriched by editorial interpretations and reconstructions of the text (to have as rich text of an inscription as possible).

Since the dataset is mostly in Latin, I did not use the following cleaning scripts: `vacat`, `editorial_notes`, `editorial_comments_latin` since they would eliminate some parts of the text that should not be eliminated. I am not using the `suppresion_superscripts_interpretive` script beacuse the structure of the EDH dataset does not contain curly braces followed by superscript numbers. The script `unclosed_brackets` has been added since EDH dataset contains a lot of unclosed brackets of all kinds. Script `substitution_edh_interpretive` was added to clean additional substitution features of the EDH dataset.

EDH has provided their own version of clean text in the column `text_cleaned` but did not provide any cleaning script or steps leading to the current state of `text_cleaned`. As a second step I will compare the output of the `interpretive_cleaning` model with the `text_cleaned` version to see who has produced better text for text mining.

The output of the function will consist of words separated by one space, so the data is ready for tidytext analysis. No interpunction will be left in the text.

```{r}
cleaning_interpretive_word_edh <- function(epigraphic_dataset){
   clean_text <- gsub(pattern=dubious_dot_subscript[1], replacement=dubious_dot_subscript[2], x=epigraphic_dataset, perl=TRUE)
   clean_text <- gsub(pattern=lacuna1[1], replacement=lacuna1[2], x=clean_text, perl=TRUE)
   clean_text <- gsub(pattern=lacuna2[1], replacement=lacuna2[2], x=clean_text, perl=TRUE)
   clean_text <- gsub(pattern=expanded_abbreviations_interpretive[1], replacement=expanded_abbreviations_interpretive[2], x=clean_text, perl=TRUE)
   clean_text <- gsub(pattern=suppresion_keep_interpretive[1], replacement=suppresion_keep_interpretive[2], x=clean_text, perl=TRUE)
   clean_text <- gsub(pattern=restoration_interpretive[1], replacement=restoration_interpretive[2], x=clean_text, perl=TRUE)
   clean_text <- gsub(pattern=substitution_edh_interpretive[1], replacement=substitution_edh_interpretive[2], x=clean_text, perl=TRUE)
   clean_text <- gsub(pattern=substitution_interpretive[1], replacement=substitution_interpretive[2], x=clean_text, perl=TRUE)
   clean_text <- gsub(pattern=new_line[1], replacement=new_line[2], x=clean_text, perl=TRUE)
   clean_text <- gsub(pattern=split_word_multiline[1], replacement=split_word_multiline[2], x=clean_text, perl=TRUE)
   clean_text <- gsub(pattern=erasure_empty[1], replacement=erasure_empty[2], x=clean_text, perl=TRUE)
   clean_text <- gsub(pattern=erasure_new_text[1], replacement=erasure_new_text[2], x=clean_text, perl=TRUE)
   clean_text <- gsub(pattern=interpunction_symbols[1], replacement=interpunction_symbols[2], x=clean_text, perl=TRUE)
   clean_text <- gsub(pattern=superscript_numbers[1], replacement=superscript_numbers[2], x=clean_text, perl=TRUE)
   clean_text <- gsub(pattern=epigraphic_symbols[1], replacement=epigraphic_symbols[2], x=clean_text, perl=TRUE)
   clean_text <- gsub(pattern=uncertainty_symbols[1], replacement=uncertainty_symbols[2], x=clean_text, perl=TRUE)
   clean_text <- gsub(pattern=end_line[1], replacement=end_line[2], x=clean_text, perl=TRUE)
   clean_text <- gsub(pattern=extra_blank[1], replacement=extra_blank[2], x=clean_text, perl=TRUE)
   clean_text <- gsub(pattern=arabic_numerals[1], replacement=arabic_numerals[2], x=clean_text, perl=TRUE)
   clean_text <- gsub(pattern=multi_whitespace[1], replacement=multi_whitespace[2], x=clean_text, perl=TRUE)
   clean_text <- gsub(pattern=whitespace_endline[1], replacement=whitespace_endline[2], x=clean_text, perl=TRUE)
      return(clean_text)
}
```

`Transcription` column of the first five inscriptions before cleaning:

```{r}
print(EDH_clean$transcription[1:5])
```

`Text_cleaned` column, provided by EDH as a clean version of the text, for comparison with the output of the ```cleaning_intepretive_word_edh``` function:

```{r}
print(EDH_clean$text_cleaned[1:5])
```

Output of the ```cleaning_interpretive_word_edh``` function:
```{r}
example_edh2 <- as.data.frame(cleaning_interpretive_word_edh(EDH_clean$transcription))
example_edh2[1:5,]
```

### Interpretive model for 'tidytext' analysis based on the analysis of sentences

*Aim:* to have a clean text enriched by editorial interpretations and reconstructions of the text (to have as rich text of an inscription as possible).

Since the dataset is mostly in Latin, I did not use the following cleaning scripts: `vacat`, `editorial_notes`, `editorial_comments_latin` since they would eliminate some parts of the text that should not be eliminated. I am not using the `suppresion_superscripts_interpretive` script beacuse the structure of the EDH dataset does not contain curly braces followed by superscript numbers. The script `unclosed_brackets` has been added since EDH dataset contains a lot of unclosed brackets of all kinds. Script `substitution_edh_interpretive` was added to clean additional substitution features of the EDH dataset.

EDH has provided their own version of clean text in the column `text_cleaned` but did not provide any cleaning script or steps leading to the current state of `text_cleaned`. As a second step I will compare the output of the `interpretive_cleaning` model with the `text_cleaned` version to see who has produced better text for text mining.

The output of the function will consist of words separated by one space, so the data is ready for tidytext analysis. Sentence separators will be left in the text, so individual sentences can be analysed separately. For this reason the block `interpunction_symbols` was substituted by `interpunction_keep_sentences`.

```{r}
cleaning_interpretive_sentence_edh <- function(epigraphic_dataset){
   clean_text <- gsub(pattern=dubious_dot_subscript[1], replacement=dubious_dot_subscript[2], x=epigraphic_dataset, perl=TRUE)
   clean_text <- gsub(pattern=lacuna1[1], replacement=lacuna1[2], x=clean_text, perl=TRUE)
   clean_text <- gsub(pattern=lacuna2[1], replacement=lacuna2[2], x=clean_text, perl=TRUE)
   clean_text <- gsub(pattern=expanded_abbreviations_interpretive[1], replacement=expanded_abbreviations_interpretive[2], x=clean_text, perl=TRUE)
   clean_text <- gsub(pattern=suppresion_keep_interpretive[1], replacement=suppresion_keep_interpretive[2], x=clean_text, perl=TRUE)
   clean_text <- gsub(pattern=restoration_interpretive[1], replacement=restoration_interpretive[2], x=clean_text, perl=TRUE)
   clean_text <- gsub(pattern=substitution_edh_interpretive[1], replacement=substitution_edh_interpretive[2], x=clean_text, perl=TRUE)
   clean_text <- gsub(pattern=substitution_interpretive[1], replacement=substitution_interpretive[2], x=clean_text, perl=TRUE)
   clean_text <- gsub(pattern=new_line[1], replacement=new_line[2], x=clean_text, perl=TRUE)
   clean_text <- gsub(pattern=split_word_multiline[1], replacement=split_word_multiline[2], x=clean_text, perl=TRUE)
   clean_text <- gsub(pattern=erasure_empty[1], replacement=erasure_empty[2], x=clean_text, perl=TRUE)
   clean_text <- gsub(pattern=erasure_new_text[1], replacement=erasure_new_text[2], x=clean_text, perl=TRUE)
   clean_text <- gsub(pattern=interpunction_keep_sentences[1], replacement=interpunction_keep_sentences[2], x=clean_text, perl=TRUE)
   clean_text <- gsub(pattern=superscript_numbers[1], replacement=superscript_numbers[2], x=clean_text, perl=TRUE)
   clean_text <- gsub(pattern=epigraphic_symbols[1], replacement=epigraphic_symbols[2], x=clean_text, perl=TRUE)
   clean_text <- gsub(pattern=uncertainty_symbols[1], replacement=uncertainty_symbols[2], x=clean_text, perl=TRUE)
   clean_text <- gsub(pattern=end_line[1], replacement=end_line[2], x=clean_text, perl=TRUE)
   clean_text <- gsub(pattern=extra_blank[1], replacement=extra_blank[2], x=clean_text, perl=TRUE)
   clean_text <- gsub(pattern=arabic_numerals[1], replacement=arabic_numerals[2], x=clean_text, perl=TRUE)
   clean_text <- gsub(pattern=multi_whitespace[1], replacement=multi_whitespace[2], x=clean_text, perl=TRUE)
   clean_text <- gsub(pattern=whitespace_endline[1], replacement=whitespace_endline[2], x=clean_text, perl=TRUE)
      return(clean_text)
}

```

`Transcription` column of three inscriptions before cleaning:

```{r}
print(EDH_clean$transcription[c(2297, 2444, 3026)])
```

`Text_cleaned` column, provided by EDH as a clean version of the text:

```{r}
print(EDH_clean$text_cleaned[c(2297, 2444, 3026)])
```

Output of the ```cleaning_interpretive_sentence_edh``` function (with interpunction):
```{r}
example_edh3 <- as.data.frame(cleaning_interpretive_sentence_edh(EDH_clean$transcription))
example_edh3[c(2297, 2444, 3026),]
```

### Enriching the full dataset with conservative and interpretive cleaned versions of the text:

```{r}
EDH_clean <- EDH_clean %>%
  mutate(clean_text_conservative = cleaning_conservative_edh(EDH_clean$transcription)) %>%
  mutate(clean_text_interpretive_word = cleaning_interpretive_word_edh(EDH_clean$transcription))  %>%
  mutate(clean_text_interpretive_sentence = cleaning_interpretive_sentence_edh(EDH_clean$transcription))
```


# Comparing the text extracted from XML files with the from the API that has been cleaned with the cleaning functions

```{r}
texts <- EDH_clean %>%
  select(text_edition, clean_text_conservative, clean_text_interpretive_word, clean_text_interpretive_sentence, transcription)
```

# Comparing qualitatively individual outputs next to each other
```{r}
textno <- 1

texts$text_edition[textno]
texts$clean_text_conservative[textno]
texts$clean_text_interpretive_word[textno]
texts$clean_text_interpretive_sentence[textno]
texts$transcription[textno]
```

# comparing the total number of texts
```{r}
length(unique(unlist(texts$text_edition)))
length(unique(texts$clean_text_conservative))
length(unique(texts$clean_text_interpretive_word))
length(unique(texts$clean_text_interpretive_sentence))
length(unique(unlist(texts$transcription)))
```

# comparing the contents of `text_edition` and `clean_text_interpretive_word`

## how many more texts of inscriptions are in `text_edition` than in `clean_text_interpretive_word`
```{r}
length(unique(unlist(texts$text_edition))) - length(unique(texts$clean_text_interpretive_word))
```

## What is the contents of clean_text_interpretive_word when text-edition inscriptions contains NULL
```{r}
texts %>% 
  select(text_edition, clean_text_interpretive_word, transcription) %>% 
  filter(clean_text_interpretive_word == "NULL") %>% 
  View()
```
## What is the contents of text-edition when clean_text_interpretive_word inscriptions contains NULL
```{r}
texts %>% 
  select(text_edition, clean_text_interpretive_word, transcription) %>% 
  filter(text_edition == "NULL") %>% 
  View()
```

## How many total words there are in in `text_edition` and in `clean_text_interpretive_word` and what is the difference
```{r}
sum(lengths(gregexpr("\\w+", texts$text_edition)) + 1) 
sum(lengths(gregexpr("\\w+", texts$clean_text_interpretive_word)) + 1) 
sum(lengths(gregexpr("\\w+", texts$transcription)) + 1) 

sum(lengths(gregexpr("\\w+", texts$text_edition)) + 1) - sum(lengths(gregexpr("\\w+", texts$clean_text_interpretive_word)) + 1)
```

## Using different method of word counting
```{r}
sum(str_count(texts$text_edition, '\\w+'))
sum(str_count(texts$clean_text_interpretive_word, '\\w+'))
sum(str_count(texts$transcription, '\\w+'))

sum(str_count(texts$text_edition, '\\w+')) - sum(str_count(texts$clean_text_interpretive_word, '\\w+'))
```


# How many times are special characters appearing in the `text_edition` (that should not be there)
```{r}
texts$text_edition %>% str_subset(pattern = "\\(") %>% unique() %>% length()
texts$text_edition %>% str_subset(pattern = "\\?") %>% unique() %>% length()
texts$text_edition %>% str_subset(pattern = "\\$") %>% unique() %>% length()
texts$text_edition %>% str_subset(pattern = "\\[") %>% unique() %>% length()
texts$text_edition %>% str_subset(pattern = "\\-") %>% unique() %>% length()
texts$text_edition %>% str_subset(pattern = "\\|") %>% unique() %>% length()
texts$text_edition %>% str_subset(pattern = "\\.") %>% unique() %>% length()
```

# How many times are special characters appearing in the `clean_text_interpretive_word` (that should not be there)
```{r}
texts$clean_text_interpretive_word %>% str_subset(pattern = "\\(") %>% unique() %>% length()
texts$clean_text_interpretive_word %>% str_subset(pattern = "\\?") %>% unique() %>% length()
texts$clean_text_interpretive_word %>% str_subset(pattern = "\\$") %>% unique() %>% length()
texts$clean_text_interpretive_word %>% str_subset(pattern = "\\[") %>% unique() %>% length()
texts$clean_text_interpretive_word %>% str_subset(pattern = "\\-") %>% unique() %>% length()
texts$clean_text_interpretive_word %>% str_subset(pattern = "\\|") %>% unique() %>% length()
texts$clean_text_interpretive_word %>% str_subset(pattern = "\\.") %>% unique() %>% length()
```












# Saving to Sciencedata
```{r}
EDH_clean_tibble <- as_tibble(EDH_clean)
EDH_cleaned_json <- jsonlite::toJSON(EDH_clean)

write(EDH_cleaned_json, file="EDH_cleaned_2020-09-16.json")
user <- readline("your sciencedata username: ")
request("EDH_cleaned_2020-09-16.json", path="/sharingout/648597@au.dk/SDAM_root/SDAM_data/EDH/",
        method="PUT", cred=c(user, getPass("your sciencedata password: ")))
```

# Remove local copy of the json
```{r}
file.remove("E./DH_cleaned.json")
#file.remove("EDH_cleaned_sample.json")
```
